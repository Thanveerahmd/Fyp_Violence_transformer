{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLVS_video_transformers_40_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thanveerahmd/Fyp_Violence_transformer/blob/main/RLVS_video_transformers_40_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19af9foWzIwP",
        "outputId": "1564a996-022a-4c57-b23a-45528a2cc241"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_4icYOKlDWi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFCbLKqk8R9O",
        "outputId": "e90661a2-fb61-4124-a226-b9df026b8009"
      },
      "source": [
        "cd /content/gdrive/MyDrive/Final_Year_Project/Dataset/RLVS/real_life_violence_situations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Final_Year_Project/Dataset/RLVS/real_life_violence_situations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YJH-c2s7vMj",
        "outputId": "93e3fd6b-5fcf-4f3b-eaa7-d5975a83c733"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " animation.gif                  rlvs_train_data_40.npy\n",
            "\u001b[0m\u001b[01;34m'Real Life Violence Dataset'\u001b[0m/   rlvs_train_data.npy\n",
            " \u001b[01;34mrlvs\u001b[0m/                          rlvs_train_labels_40.npy\n",
            " rlvs_test_data_40.npy          rlvs_train_labels.npy\n",
            " rlvs_test_data.npy             rlvs_train.txt\n",
            " rlvs_test_labels_40.npy        test.csv\n",
            " rlvs_test_labels.npy           train.csv\n",
            " rlvs_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpzGCdAAFXDB",
        "outputId": "a7802676-c8c1-4c86-9773-fb3eb100be40"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpCKZrI84jH8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NOE7tRL4jH8"
      },
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVghqK14jH8"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp0-XjyQ4jH9"
      },
      "source": [
        "MAX_SEQ_LENGTH = 40\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 200\n",
        "\n",
        "EPOCHS = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Qmetr04jH9"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "We will mostly be following the same data preparation steps in this example, except for\n",
        "the following changes:\n",
        "\n",
        "* We reduce the image size to 128x128 instead of 224x224 to speed up computation.\n",
        "* Instead of using a pre-trained [InceptionV3](https://arxiv.org/abs/1512.00567) network,\n",
        "we use a pre-trained\n",
        "[DenseNet121](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n",
        "for feature extraction.\n",
        "* We directly pad shorter videos to length `MAX_SEQ_LENGTH`.\n",
        "\n",
        "First, let's load up the\n",
        "[DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTpXzBBc4jH-",
        "outputId": "40e8d6a2-282c-4cfb-bdce-97abaccee287"
      },
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    width  = cap.get(3)  # float `width`\n",
        "    height = cap.get(4)  # float `height`\n",
        "    print('width, height:', width, height)\n",
        "\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            #frame = crop_center(frame)\n",
        "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        #print(path)\n",
        "        print(frames.shape)\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 1600\n",
            "Total videos for testing: 400\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n",
            "['NonViolence', 'Violence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlToYjdXGEd_"
      },
      "source": [
        "train_data, train_labels = prepare_all_videos(train_df, \"rlvs/train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"rlvs/test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in test set: {test_data[1].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sHvofITHCvG"
      },
      "source": [
        "from numpy import save\n",
        "\n",
        "save('rlvs_train_data.npy', train_data)\n",
        "save('rlvs_train_labels.npy', train_labels)\n",
        "save('rlvs_test_data.npy', test_data)\n",
        "save('rlvs_test_labels.npy', test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpCqB_laIWl"
      },
      "source": [
        "train_data, train_labels = np.load(\"rlvs_train_data_40.npy\"), np.load(\"rlvs_train_labels_40.npy\")\n",
        "test_data, test_labels = np.load(\"rlvs_test_data_40.npy\"), np.load(\"rlvs_test_labels_40.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJTe-4ebTU5",
        "outputId": "4be32ff1-2381-4788-ee8d-b5f4d48c9550"
      },
      "source": [
        "print(f\"Frame features in train set: {train_data.shape}\")\n",
        "print(f\"Frame features in train label set: {train_labels.shape}\")\n",
        "print(f\"Frame features in test set: {test_data.shape}\")\n",
        "print(f\"Frame features in test label set: {test_labels.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (1600, 40, 1024)\n",
            "Frame features in train label set: (1600, 1)\n",
            "Frame features in test set: (400, 40, 1024)\n",
            "Frame features in test label set: (400, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccvbWoM14jH_"
      },
      "source": [
        "Calling `prepare_all_videos()` on `train_df` and `test_df` takes ~20 minutes to\n",
        "complete. For this reason, to save time, here we download already preprocessed NumPy arrays:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKgeja3c4jIA"
      },
      "source": [
        "## Building the Transformer-based model\n",
        "\n",
        "We will be building on top of the code shared in\n",
        "[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11) of\n",
        "[Deep Learning with Python (Second ed.)](https://www.manning.com/books/deep-learning-with-python)\n",
        "by François Chollet.\n",
        "\n",
        "First, self-attention layers that form the basic blocks of a Transformer are\n",
        "order-agnostic. Since videos are ordered sequences of frames, we need our\n",
        "Transformer model to take into account order information.\n",
        "We do this via **positional encoding**.\n",
        "We simply embed the positions of the frames present inside videos with an\n",
        "[`Embedding` layer](https://keras.io/api/layers/core_layers/embedding). We then\n",
        "add these positional embeddings to the precomputed CNN feature maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyimqvcw4jIB"
      },
      "source": [
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DCmGiB4jIB"
      },
      "source": [
        "Now, we can create a subclassed layer for the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiLzde8R4jIB"
      },
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JerMOL-Y5XPE"
      },
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 64\n",
        "    num_heads = 8\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(embed_dim,return_sequences=True))(x)\n",
        "    x = TransformerEncoder(2*embed_dim, dense_dim, num_heads, name=\"transformer_layer_1\")(x)\n",
        "    #x = TransformerEncoder(2*embed_dim, dense_dim, num_heads, name=\"transformer_layer_2\")(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(2*embed_dim,return_sequences=True))(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    \n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24eKqWlc5dP-"
      },
      "source": [
        "model = get_compiled_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrwvjtxO5e93",
        "outputId": "16fd1285-90ce-43ef-bc59-82ec69db596b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None, None)]      0         \n",
            "_________________________________________________________________\n",
            "frame_position_embedding (Po (None, None, 1024)        40960     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, None, 2048)        16785408  \n",
            "_________________________________________________________________\n",
            "transformer_layer_1 (Transfo (None, None, 2048)        134541376 \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, None, 4096)        67125248  \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 218,497,089\n",
            "Trainable params: 218,497,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz7fSHb54jIC"
      },
      "source": [
        "## Utility functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEtiov4T4jIC"
      },
      "source": [
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    \n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model,history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRzwtbc94jID"
      },
      "source": [
        "## Model training and inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mBfjP284jIE",
        "outputId": "079f4abb-fe94-4067-fa9d-6694ccf587d8"
      },
      "source": [
        "trained_model,history = run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "43/43 [==============================] - 71s 1s/step - loss: 1.2487 - accuracy: 0.5316 - val_loss: 1.0696 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.06964, saving model to /tmp/video_classifier\n",
            "Epoch 2/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6855 - accuracy: 0.5772 - val_loss: 0.8440 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.06964 to 0.84405, saving model to /tmp/video_classifier\n",
            "Epoch 3/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6925 - accuracy: 0.5684 - val_loss: 0.8057 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84405 to 0.80567, saving model to /tmp/video_classifier\n",
            "Epoch 4/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6926 - accuracy: 0.5596 - val_loss: 1.0490 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.80567\n",
            "Epoch 5/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6871 - accuracy: 0.5721 - val_loss: 0.7360 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80567 to 0.73600, saving model to /tmp/video_classifier\n",
            "Epoch 6/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.6979 - accuracy: 0.5574 - val_loss: 0.9383 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.73600\n",
            "Epoch 7/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6668 - accuracy: 0.6110 - val_loss: 0.6985 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.73600 to 0.69853, saving model to /tmp/video_classifier\n",
            "Epoch 8/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6994 - accuracy: 0.5507 - val_loss: 0.8454 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69853\n",
            "Epoch 9/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6943 - accuracy: 0.5632 - val_loss: 0.7166 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69853\n",
            "Epoch 10/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6929 - accuracy: 0.5551 - val_loss: 0.8640 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69853\n",
            "Epoch 11/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6886 - accuracy: 0.5485 - val_loss: 1.0414 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69853\n",
            "Epoch 12/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6796 - accuracy: 0.5919 - val_loss: 0.9031 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69853\n",
            "Epoch 13/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6896 - accuracy: 0.5654 - val_loss: 1.0039 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69853\n",
            "Epoch 14/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6878 - accuracy: 0.5846 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69853 to 0.61035, saving model to /tmp/video_classifier\n",
            "Epoch 15/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6828 - accuracy: 0.5618 - val_loss: 0.9790 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.61035\n",
            "Epoch 16/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6895 - accuracy: 0.5750 - val_loss: 1.0852 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.61035\n",
            "Epoch 17/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6852 - accuracy: 0.5669 - val_loss: 0.9595 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.61035\n",
            "Epoch 18/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6884 - accuracy: 0.5757 - val_loss: 0.8135 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.61035\n",
            "Epoch 19/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6880 - accuracy: 0.5743 - val_loss: 0.9198 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.61035\n",
            "Epoch 20/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6867 - accuracy: 0.5757 - val_loss: 0.6753 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.61035\n",
            "Epoch 21/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6882 - accuracy: 0.5721 - val_loss: 0.8097 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.61035\n",
            "Epoch 22/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6823 - accuracy: 0.5787 - val_loss: 0.9372 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.61035\n",
            "Epoch 23/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6893 - accuracy: 0.5721 - val_loss: 0.8450 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.61035\n",
            "Epoch 24/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6986 - accuracy: 0.5478 - val_loss: 1.0343 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.61035\n",
            "Epoch 25/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.6405 - accuracy: 0.6368 - val_loss: 0.5315 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.61035 to 0.53146, saving model to /tmp/video_classifier\n",
            "Epoch 26/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.4333 - accuracy: 0.8419 - val_loss: 0.4664 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.53146 to 0.46638, saving model to /tmp/video_classifier\n",
            "Epoch 27/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2818 - accuracy: 0.8926 - val_loss: 0.9004 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.46638\n",
            "Epoch 28/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.3017 - accuracy: 0.8750 - val_loss: 1.5893 - val_accuracy: 0.4375\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.46638\n",
            "Epoch 29/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2053 - accuracy: 0.9257 - val_loss: 0.1958 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.46638 to 0.19585, saving model to /tmp/video_classifier\n",
            "Epoch 30/50\n",
            "43/43 [==============================] - 52s 1s/step - loss: 0.2393 - accuracy: 0.9051 - val_loss: 0.5411 - val_accuracy: 0.8292\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.19585\n",
            "Epoch 31/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2114 - accuracy: 0.9287 - val_loss: 0.9757 - val_accuracy: 0.6708\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.19585\n",
            "Epoch 32/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1778 - accuracy: 0.9309 - val_loss: 0.2737 - val_accuracy: 0.8708\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.19585\n",
            "Epoch 33/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2252 - accuracy: 0.9132 - val_loss: 0.3070 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.19585\n",
            "Epoch 34/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1800 - accuracy: 0.9382 - val_loss: 0.3391 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.19585\n",
            "Epoch 35/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1559 - accuracy: 0.9463 - val_loss: 0.4866 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.19585\n",
            "Epoch 36/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1281 - accuracy: 0.9566 - val_loss: 0.1615 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.19585 to 0.16150, saving model to /tmp/video_classifier\n",
            "Epoch 37/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1176 - accuracy: 0.9632 - val_loss: 0.9841 - val_accuracy: 0.7167\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.16150\n",
            "Epoch 38/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1320 - accuracy: 0.9456 - val_loss: 0.4781 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.16150\n",
            "Epoch 39/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1001 - accuracy: 0.9713 - val_loss: 0.8405 - val_accuracy: 0.8167\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.16150\n",
            "Epoch 40/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1378 - accuracy: 0.9566 - val_loss: 0.4352 - val_accuracy: 0.8708\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.16150\n",
            "Epoch 41/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0840 - accuracy: 0.9713 - val_loss: 0.2860 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.16150\n",
            "Epoch 42/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0919 - accuracy: 0.9750 - val_loss: 1.1706 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.16150\n",
            "Epoch 43/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.1341 - accuracy: 0.9515 - val_loss: 0.5686 - val_accuracy: 0.8792\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.16150\n",
            "Epoch 44/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2112 - accuracy: 0.9279 - val_loss: 0.5920 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.16150\n",
            "Epoch 45/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.2792 - accuracy: 0.9000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.16150 to 0.00313, saving model to /tmp/video_classifier\n",
            "Epoch 46/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0984 - accuracy: 0.9706 - val_loss: 0.2345 - val_accuracy: 0.9542\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00313\n",
            "Epoch 47/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0869 - accuracy: 0.9757 - val_loss: 0.1630 - val_accuracy: 0.9500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00313\n",
            "Epoch 48/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0610 - accuracy: 0.9846 - val_loss: 0.0497 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00313\n",
            "Epoch 49/50\n",
            "43/43 [==============================] - 49s 1s/step - loss: 0.0776 - accuracy: 0.9713 - val_loss: 0.3693 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00313\n",
            "Epoch 50/50\n",
            "43/43 [==============================] - 50s 1s/step - loss: 0.1485 - accuracy: 0.9456 - val_loss: 1.2303 - val_accuracy: 0.6125\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00313\n",
            "13/13 [==============================] - 5s 386ms/step - loss: 1.1694 - accuracy: 0.7850\n",
            "Test accuracy: 78.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umkyad5A_SVa"
      },
      "source": [
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4ToDGq8upF"
      },
      "source": [
        "    pyplot.title('Learning Curves')\n",
        "    pyplot.xlabel('Epoch')\n",
        "    pyplot.ylabel('Root Mean Squared Error')\n",
        "    pyplot.plot(history.history['loss'], label='train')\n",
        "    pyplot.plot(history.history['val_loss'], label='val')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7Bs8PJ1l4iZ"
      },
      "source": [
        "    pyplot.title('Learning Curves')\n",
        "    pyplot.xlabel('Epoch')\n",
        "    pyplot.ylabel('Root Mean Squared Error')\n",
        "    pyplot.plot(history.history['accuracy'], label='train')\n",
        "    pyplot.plot(history.history['val_accuracy'], label='val')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dUgVkb4jIE"
      },
      "source": [
        "**Note**: This model has ~4.23 Million parameters, which is way more than the sequence\n",
        "model (99918 parameters) we used in the prequel of this example.  This kind of\n",
        "Transformer model works best with a larger dataset and a longer pre-training schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXnnyqSF4jIF"
      },
      "source": [
        "\n",
        "def prepare_single_video(frames):\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    # Pad shorter videos.\n",
        "    if len(frames) < MAX_SEQ_LENGTH:\n",
        "        diff = MAX_SEQ_LENGTH - len(frames)\n",
        "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "        frames = np.concatenate(frames, padding)\n",
        "\n",
        "    frames = frames[None, ...]\n",
        "\n",
        "    # Extract features from the frames of the current video.\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            if np.mean(batch[j, :]) > 0.0:\n",
        "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "            else:\n",
        "                frame_features[i, j, :] = 0.0\n",
        "\n",
        "    return frame_features\n",
        "\n",
        "\n",
        "def predict_action(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"rlvs/test/\", path))\n",
        "    frame_features = prepare_single_video(frames)\n",
        "    probabilities = trained_model.predict(frame_features)[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "# This utility is for visualization.\n",
        "# Referenced from:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def to_gif(images):\n",
        "    converted_images = images.astype(np.uint8)\n",
        "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
        "    return embed.embed_file(\"animation.gif\")\n",
        "\n",
        "\n",
        "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "print(f\"Test video path: {test_video}\")\n",
        "test_frames = predict_action(test_video)\n",
        "to_gif(test_frames[:MAX_SEQ_LENGTH])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaS-e7VB4jIG"
      },
      "source": [
        "The performance of our model is far from optimal, because it was trained on a\n",
        "small dataset."
      ]
    }
  ]
}